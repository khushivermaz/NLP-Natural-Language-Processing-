{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "corpus = [\n",
    "    'Natural language processing with word vectors is interesting',\n",
    "    'Word embeddings capture relationships between words',\n",
    "    'PCA helps reduce the dimensions of word vectors',\n",
    "    'Visualizing word vectors reveals patterns in text data',\n",
    "    'Machine learning models can be applied to natural language tasks'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text: Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # +1 for padding\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model\n",
    "embedding_dim = 50  # Size of the word vectors\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=padded_sequences.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (even though it's a dummy task, we need to initialize embeddings)\n",
    "model.fit(padded_sequences, np.zeros((padded_sequences.shape[0], 1)), epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned word embeddings\n",
    "embedding_layer = model.layers[0]\n",
    "word_embeddings = embedding_layer.get_weights()[0]  # The embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions using PCA\n",
    "def plot_word_vectors_2d(embeddings, word_index):\n",
    "    # Extract word vectors for each word in the vocabulary\n",
    "    words = list(word_index.keys())\n",
    "    vectors = np.array([embeddings[word_index[word]] for word in words])\n",
    "    # Apply PCA to reduce vector dimensions to 2\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(vectors)\n",
    "\n",
    "    # Create a scatter plot of the PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(result[:, 0], result[:, 1], edgecolors='k', c='r', s=100)\n",
    "\n",
    "    # Annotate points with words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "    \n",
    "    plt.title(\"Word Vectors Visualized with PCA (Keras Embeddings)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the word vectors in 2D\n",
    "plot_word_vectors_2d(word_embeddings, word_index)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
