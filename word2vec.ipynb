{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-step:\n",
    "* Preprocessing the data: Tokenize the text and clean it.\n",
    "* Train Word2Vec model: Use the Word2Vec algorithm from the gensim library to capture word dependencies.\n",
    "* Apply PCA: Reduce the dimensions of word vectors to two dimensions using PCA.\n",
    "Visualize the relationships: Plot the two-dimensional word vectors using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    'Natural language processing with word vectors is interesting',\n",
    "    'Word embeddings capture relationships between words',\n",
    "    'PCA helps reduce the dimensions of word vectors',\n",
    "    'Visualizing word vectors reveals patterns in text data',\n",
    "    'Machine learning models can be applied to natural language tasks'\n",
    "]\n",
    "\n",
    "# Preprocessing the corpus\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the entire corpus\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extract the word vectors\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# Reduce dimensions using PCA\n",
    "def plot_word_vectors_2d(model):\n",
    "    # Get words and their corresponding vectors\n",
    "    words = list(model.wv.index_to_key)\n",
    "    vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "    # Apply PCA to reduce the vector dimensions to 2\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(vectors)\n",
    "\n",
    "    # Create a scatter plot of the PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(result[:, 0], result[:, 1], edgecolors='k', c='r', s=100)\n",
    "\n",
    "    # Annotate points with words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "    \n",
    "    plt.title(\"Word Vectors Visualized with PCA\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize word vectors in 2D\n",
    "plot_word_vectors_2d(word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Required libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Preprocessing: The corpus is tokenized, lowercased, and cleaned of punctuation.\n",
    "Word2Vec Model: The gensim library is used to train a Word2Vec model, which captures word dependencies.\n",
    "PCA: The dimensionality of word vectors is reduced to two dimensions using PCA from sklearn.\n",
    "Visualization: A scatter plot is created where each point represents a word, with annotations showing the word itself.\n",
    "This code trains word vectors and visualizes them in 2D, effectively capturing the relationships between words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    'Natural language processing with word vectors is interesting',\n",
    "    'Word embeddings capture relationships between words',\n",
    "    'PCA helps reduce the dimensions of word vectors',\n",
    "    'Visualizing word vectors reveals patterns in text data',\n",
    "    'Machine learning models can be applied to natural language tasks'\n",
    "]\n",
    "\n",
    "# Preprocess text: Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # +1 for padding\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create an embedding model\n",
    "embedding_dim = 50  # Size of the word vectors\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=padded_sequences.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (even though it's a dummy task, we need to initialize embeddings)\n",
    "model.fit(padded_sequences, np.zeros((padded_sequences.shape[0], 1)), epochs=10, verbose=0)\n",
    "\n",
    "# Extract the learned word embeddings\n",
    "embedding_layer = model.layers[0]\n",
    "word_embeddings = embedding_layer.get_weights()[0]  # The embedding matrix\n",
    "\n",
    "# Reduce dimensions using PCA\n",
    "def plot_word_vectors_2d(embeddings, word_index):\n",
    "    # Extract word vectors for each word in the vocabulary\n",
    "    words = list(word_index.keys())\n",
    "    vectors = np.array([embeddings[word_index[word]] for word in words])\n",
    "\n",
    "    # Apply PCA to reduce vector dimensions to 2\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(vectors)\n",
    "\n",
    "    # Create a scatter plot of the PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(result[:, 0], result[:, 1], edgecolors='k', c='r', s=100)\n",
    "\n",
    "    # Annotate points with words\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "    \n",
    "    plt.title(\"Word Vectors Visualized with PCA (Keras Embeddings)\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the word vectors in 2D\n",
    "plot_word_vectors_2d(word_embeddings, word_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
